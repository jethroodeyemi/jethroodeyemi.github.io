---
title: "Supervised Learning Showdown: kNN, SVM, Neural Networks, and Boosted Trees"
date: 2025-03-08
permalink: /posts/2025/03/supervised-learning-analysis/
tags:
  - machine learning
  - supervised learning
  - knn
  - svm
  - neural network
  - decision trees
  - python
---

In this post, we dive into the world of supervised learning, comparing the performance of four popular algorithms: k-Nearest Neighbors (kNN), Support Vector Machines (SVM), Neural Networks (NN), and Decision Trees with Boosting (specifically, AdaBoost). We'll analyze their effectiveness on two distinct datasets, highlighting their strengths and weaknesses.

## Datasets

We worked with two datasets in this analysis:

### Dataset 1: Fetal Health Classification

This dataset focuses on fetal health monitoring during pregnancy and childbirth. It includes 21 features extracted from Cardiotocography (CTG) exams, such as fetal heart rate and uterine contractions. The goal is to classify fetal health into three classes: Normal, Suspect, and Pathological. This dataset comprises 2,126 instances and features an imbalanced class distribution, with the Normal class having a much larger representation than other classes. This presents challenges for some models.

![Histograms of various features from the fetal health dataset](https://jethroodeyemi.github.io/files/2025_01_12_post/fig1.png)

As seen in _**[Figure 1]**_, several features exhibit skewed distributions and the target variable, _fetal_health_, has an imbalanced class distribution. Furthermore, the histogram_width feature has very high variance, so it is excluded from the modeling to simplify them.

### Dataset 2: Sleep Health and Lifestyle Dataset

The second dataset explores the link between sleep habits and lifestyle factors. It contains data from 400 individuals with 13 features such as sleep duration, physical activity, stress levels, and cardiovascular metrics. This dataset also includes information on sleep disorders, which serves as our target variable. We converted the target variable into a binary classification problem: presence of any sleep disorder (Insomnia or Sleep Apnea) vs. absence of a disorder. This resulted in a more balanced dataset. The original target included 3 categories: none, insomnia, sleep apnea.

_**[Insert Figure 3 Here: Distribution of sleep disorders]**_

_**[Figure 3]**_ depicts the distribution of sleep disorders within this dataset. The "No Disorder" category is the largest, with fewer instances of Sleep Apnea and Insomnia.

Several categorical features in this dataset were one-hot encoded to create binary variables in order to work well with the models.

## Hypotheses

Before diving into the implementation, we established a few key hypotheses:

- **Sample Size Impact:** We expected models trained on the larger fetal health dataset (Dataset 1) to exhibit better generalization performance and be less prone to overfitting compared to models trained on the sleep dataset (Dataset 2).
- **Minkowski Distance in kNN:** We predicted a higher optimal Minkowski distance (p) value for the kNN model on Dataset 2 due to the higher dimensionality resulting from one-hot encoding of the categorical features.

## Methodology

Our methodology involved the following steps:

1.  **Data Preprocessing:** We performed an 80:20 train-test split on both datasets.
2.  **Model Implementation:** We used scikit-learn, PyTorch, and torchvision to implement our models.
3.  **Baseline Model Training:** Initially, all models were trained using default parameters.
4.  **Learning Curve Analysis:** We analyzed the learning curves to identify issues such as overfitting or underfitting.
5.  **Hyperparameter Tuning:** Using a 3-fold cross-validation approach, we optimized the hyperparameters of each model.
6.  **Final Model Evaluation:** The best-performing models from the cross-validation process were evaluated on the held-out test set.

_**[Insert Figure 4 Here: The experimental process flow chart]**_

_**[Figure 4]**_ depicts the experiment process flow, from processed data to the model evaluation.

## Model Implementation

### k-Nearest Neighbors (kNN)

For kNN, we tuned the number of neighbors (k) using a rule of thumb, then cross-validated against different values. We also experimented with different Minkowski distance metrics using different p values.

### Support Vector Machine (SVM)

We explored different kernel functions (linear, polynomial, radial basis function) and regularization parameters during hyperparameter tuning, aiming to balance bias-variance trade-off.

### Neural Network (NN)

We started with a basic two-layer NN and then experimented with different architectures (number of layers, neurons, activation functions), as well as dropout and learning rate.

### Decision Trees with Boosting (AdaBoost)

We implemented AdaBoost and then optimized hyperparameters like number of estimators, learning rate, and maximum tree depth. We also implemented pre-pruning to improve generalization and reduce overfitting.

## Results

_**[Insert Table V Here: Table summarizing performance metrics with default parameters]**_

_**[Table V]**_ summarizes the performance metrics, such as accuracy, precision, and recall for all models with their default settings.

_**[Insert Figure 5 Here: Learning curves of the models with default parameters]**_

_**[Figure 5]**_ visualizes the learning curves for the models with default parameters across both datasets. In the plot, red lines indicate the training score, and green lines indicate cross validation score. Shaded areas also represent the variance.

### Hyperparameter Tuning and Cross-Validation

_**[Insert Table VI Here: Table summarizing the optimal hyperparameters]**_

_**[Table VI]**_ reports the tuned hyperparameters of each model, while, _**[Insert Figure 6 Here: Validation curves for the kNN]**_, _**[Insert Figure 7 Here: Learning Curves for the SVM model]**_, _**[Insert Figure 8 Here: Validation curves for the NN model]**_, and _**[Insert Figure 9 Here: Validation curves for the Boosted DT model]**_ illustrates the impact of different hyperparameters.

_**[Insert Figure 10 Here: Learning Curves of the models after hyper parameter tuning]**_

_**[Figure 10]**_ illustrates the learning curves of the models after hyperparameter tuning.

Key findings from hyperparameter tuning:

- **kNN:** A small number of neighbors (4) was optimal for the kNN model. The optimal Minkowski distance was 1 for Dataset 1 and 5 for Dataset 2, which supports our initial hypothesis of needing a larger p-value for the higher-dimensional Dataset 2. kNN was very computationally efficient.
- **SVM:** The RBF kernel and a regularization parameter (C) of 10 produced the best results on Dataset 1 while a C value of 1 was optimal for Dataset 2.
- **NN:** For Dataset 1, a three-layer network with (64, 32, 16) units, a dropout rate of 0.2, and a learning rate of 0.1 was ideal. On Dataset 2, the optimal setup was a single layer of 32 hidden neurons with no dropout, and a lower learning rate of 0.01.
- **Boosted DT:** We pre-pruned our boosted decision trees using parameter tuning with a depth of 2 for Dataset 1 and 1 for Dataset 2, and a lower number of samples required for a node to be a leaf.

_**[Insert Table VII Here: Table summarizing performance metrics with best parameters]**_

_**[Table VII]**_ shows the performance metrics using the optimal hyperparameters. Boosted Decision Trees reached a 93% accuracy and precision on the Fetal Health dataset and a 92% accuracy and precision on the Sleep Health Dataset, with Support Vector Machines coming in a close second. _**[Insert Table VIII Here: Table summarizing training times]**_ reports the fitting times (in seconds) for the models. The kNN model demonstrated the fastest training times, while NNs took significantly longer.

## Discussion

The Boosted Decision Tree demonstrated the highest accuracy on both datasets, however, it was prone to overfitting. The Support Vector Machine achieved comparable results, but maintained better generalization on the two different datasets by using the RBF kernel. The kNN model was also effective and computationally very efficient, but with lower accuracy in some cases. Finally, the Neural Network performed well after hyperparameter tuning, but required more computational resources.

Our initial hypotheses were confirmed as follows:

1.  **Sample Size Impact**: Models generally performed better on Dataset 1 due to its larger size. However, complex models like NN, initially overfitted to the data before they were fine-tuned with the right hyper-parameters.
2.  **Minkowski Distance in kNN:** As hypothesized, a higher p-value (5 compared to 1 for the simpler dataset 1) for the Minkowski distance metric was optimal on Dataset 2.

## Conclusion

In this project, we thoroughly examined four different machine learning models using two different datasets. We explored the impact of data size, hyperparameter tuning, and cross-validation. This analysis provides insight into how to choose and optimize a suitable model for different problems and datasets.

By evaluating different machine learning models using different metrics, we were able to reach key insights about the performance, effectiveness, and challenges involved in real-world machine learning applications.

## References

[Include references from the original document here]

---
